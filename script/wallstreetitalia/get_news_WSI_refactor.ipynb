{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import concurrent.futures\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input df\n",
    "data = pd.read_csv(\"wsi_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "MAX_THREADS = multiprocessing.cpu_count()\n",
    "BUCKET_SIZE = len(data[\"news_link\"]) / (MAX_THREADS * 8)\n",
    "OUTPUT_FILE = \"result.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_row(link):\n",
    "    \"\"\"\n",
    "        Function to scrape WSI that takes in input a link a WSI news link and return a dict with getting section, tag,                              category, description\n",
    "    \"\"\"\n",
    "    response = http.request(\"GET\",link)\n",
    "    soup = BeautifulSoup(response.data, 'html.parser')\n",
    "\n",
    "    row_dict = { \"link\": link,\n",
    "        \"section\": [ item.get(\"content\", None) for item in soup.findAll(\"meta\", { \"property\" : re.compile(r\"section\")}) ],\n",
    "        \"tag\": [ item.get(\"content\", None) for item in soup.findAll(\"meta\", { \"property\" : re.compile(r\"tag\")}) ],\n",
    "        \"category\": [item.text for item in soup.find(\"div\", {\"class\":\"category\"}).findAll(\"li\") ],\n",
    "        \"description\": soup.find(\"meta\", {\"name\": re.compile(r\"description\")}).get(\"content\", None)\n",
    "        }\n",
    "    \n",
    "    return row_dict\n",
    "    \n",
    "\n",
    "\n",
    "def exec_threads(function, serie, file_name):\n",
    "    \"\"\"\n",
    "        Function that divide the execution in threads and save the output that takes 3 input: a scrape function, a pd links serie, and a            csv filename\n",
    "    \"\"\"\n",
    "    threads = min(MAX_THREADS, len(serie))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        result = pd.DataFrame(executor.map(function, serie)).to_csv(file_name, mode='a', header=False)\n",
    "\n",
    "\n",
    "\n",
    "def main(urls):\n",
    "\n",
    "    t0 = time.time()\n",
    "    exec_threads(get_row,urls,OUTPUT_FILE)\n",
    "    t1 = time.time()\n",
    "    print(f\"---- {t1-t0} seconds ---- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"executing in {MAX_THREADS} threads \")\n",
    "### divide the execution in bucket\n",
    "for serie in np.split(data[\"news_link\"], BUCKET_SIZE):\n",
    "    main(serie)"
   ]
  }
 ]
}